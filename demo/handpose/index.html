<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Handpose Demo</title>

    <!-- Require the peer dependencies of handpose. -->
    <script src="./vendor/tf-core.js"></script>
    <script src="./vendor/tf-converter.js"></script>

    <!-- You must explicitly require a TF.js backend if you're not using the tfs union bundle. -->
    <script src="./vendor/tf-backend-webgl.js"></script>
    <!-- Alternatively you can use the WASM backend: <script src="https://unpkg.com/@tensorflow/tfjs-backend-wasm@2.1.0/dist/tf-backend-wasm.js"></script> -->

    <script src="./vendor/handpose.js"></script>
</head>

<body>
    <video id="video" playsinline=""
        style="-webkit-transform:scaleX(-1);transform:scaleX(-1);width:auto;height:auto;position:absolute;">
    </video>
    <script>
        // Prefer camera resolution nearest to 1280x720.
        var constraints = { audio: false, video: { width: 1280, height: 720 } };

        navigator.mediaDevices.getUserMedia(constraints)
            .then(function (mediaStream) {
                var video = document.querySelector('video');
                video.srcObject = mediaStream;
                video.onloadedmetadata = function (e) {
                    video.play();
                    main();
                };
            })
            .catch(function (err) { console.log(err.name + ": " + err.message); }); // always check for errors at the end.
        
        async function main() {
            // Load the MediaPipe handpose model.
            const model = await handpose.load();
            // Pass in a video stream (or an image, canvas, or 3D tensor) to obtain a
            // hand prediction from the MediaPipe graph.
            const predictions = await model.estimateHands(document.querySelector("video"));
            if (predictions.length > 0) {
                /*
                `predictions` is an array of objects describing each detected hand, for example:
                [
                  {
                    handInViewConfidence: 1, // The probability of a hand being present.
                    boundingBox: { // The bounding box surrounding the hand.
                      topLeft: [162.91, -17.42],
                      bottomRight: [548.56, 368.23],
                    },
                    landmarks: [ // The 3D coordinates of each hand landmark.
                      [472.52, 298.59, 0.00],
                      [412.80, 315.64, -6.18],
                      ...
                    ],
                    annotations: { // Semantic groupings of the `landmarks` coordinates.
                      thumb: [
                        [412.80, 315.64, -6.18]
                        [350.02, 298.38, -7.14],
                        ...
                      ],
                      ...
                    }
                  }
                ]
                */

                for (let i = 0; i < predictions.length; i++) {
                    const keypoints = predictions[i].landmarks;

                    // Log hand keypoints.
                    for (let i = 0; i < keypoints.length; i++) {
                        const [x, y, z] = keypoints[i];
                        console.log(`Keypoint ${i}: [${x}, ${y}, ${z}]`);
                    }
                }
            }else{
                console.log('no result!')
            }
        }
    </script>
</body>

</html>